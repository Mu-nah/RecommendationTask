Take-Home Assignment – ONYEBUKWA HILARY MUNACHISO

Engagement score Design 
Formula (high-level)
For each event on a ping, I assigned an event-level score and then aggregate per (user, ping).
•	view contributes: 1.0 * watch_time_ratio where watch_time_ratio = watch_time_sec / duration_sec. (So full watch ~1.0, half watch ~0.5.)
•	like contributes +2.0
•	comment contributes +3.0
•	share contributes +4.0
•	follow_creator contributes +2.0
•	impression contributes +0.1 (very weak signal)
Per (user, ping) engagement score = sum of all event-scores from that user for that ping.
Global ping score = sum of (user,ping) engagement scores across all users.
Rationale 
Views are a core engagement signal but must be scaled by how much of the video was actually watched — so we use watch_time_ratio. Stronger actions (comment, share) indicate higher intent and should be weighted heavier than likes. Impressions are weak signals and only give a tiny positive contribution (to avoid zeroing out cold-start items). Follow-creator is a strong signal for creator affinity and gets a moderate boost.

Python (pandas) code — see full pipeline on pingtop_recs.py
This single script:
•	reads users.csv, pings.csv, interactions.csv,
•	cleans/pads missing durations if needed,
•	computes watch_time_ratio, event-level event_score, per (user, ping) engagement, global ping score,
•	prints/saves top-10 pings, watch_time_ratio distribution, new-vs-existing summary,
•	builds a simple heuristic recommender and outputs Top-10 for 3 example users


the output directory will contain:
•	ping_global.csv (global engagement per ping),
•	user_ping.csv (per-user/ping engagement),
•	top10_pings.csv,
•	watch_time_ratio_stats.csv,
•	avg_watch_by_group.csv,
•	avg_pings_by_group.csv,
•	recs/ with recs_u1.csv…..
what the metrics will tell you
•	Many view events have low watch_time_ratio (median often < 0.5) — suggests frequent skims/swipes; prioritize short hooks or higher early retention.
•	Items with high global engagement frequently combine many partial/full views and some likes/shares; shares/comments are rare but amplify item score.
•	New users (signed up in last 7 days) typically have lower absolute number of interactions but sometimes higher watch ratios (onboarding or novelty effect) — consider showing more engaging/popular content early.
•	If impressions dominate (lots of impressions, few views), feed freshness and strong thumbnails might help lift view rates.
•	Creator-follow events are valuable signals for personalized ranking (use to bias towards that creator’s content).
Recommendation Prototype — logic + why
Approach chosen: Heuristic ranking (simple linear combination).
Score = α * global_popularity + β * user_category_affinity + γ * user_creator_affinity + δ * freshness.
•	global_popularity = normalized global engagement (fast, low-cost).
•	user_category_affinity = fraction of a user's historical interactions that hit that category.
•	user_creator_affinity = same for creator.
•	freshness = simple inverse of age-in-days (encourages recent content).

Why: Simple, explainable, cheap to compute, parallelizable, good baseline for warm-start and A/B tests before investing/ before transitioning to more advanced models such as Youtube two tower, XGBoost, LightGBM, or transformer-based architectures
Feature building notes:
•	Compute affinities from last N days (sliding window) to capture shifting tastes.
•	Normalize features to [0,1].
Example Top-10 output format
Below are the Top-10 lists for three users in the requested CSV-like format (these match sample output).
User_1 Recommendations
ping_id	score	category	main_hashtag	creator_id	reason
p20	0.5818025967477625	education	travel	u14	prefers cat=education; engaged this creator; globally popular; recent
p25	0.5285873356023362	beauty	makeup	u11	prefers cat=beauty; globally popular; recent
p7	0.5009713293275627	travel	music	u8	globally popular; recent
p29	0.49985953791575877	dance	food	u19	prefers cat=dance; globally popular; recent
p32	0.44234448549335725	comedy	pets	u7	globally popular; recent
p36	0.43954578900618263	dance	music	u17	prefers cat=dance; engaged this creator; globally popular; recent
p17	0.41814190219903286	travel	comedy	u17	engaged this creator; globally popular; recent
p13	0.36826325369294877	food	makeup	u18	engaged this creator; globally popular; recent
p33	0.36046863524488626	dance	fitness	u16	prefers cat=dance; globally popular; recent
p1	0.34329027930345213	education	football	u3	prefers cat=education; globally popular; recent


User_2 Recommendations
ping_id	score	category	main_hashtag	creator_id	reason
p30	0.5303472222222222	gaming	pets	u14	engaged this creator; globally popular; recent
p25	0.5098373356023362	beauty	makeup	u11	prefers cat=beauty; globally popular; recent
p32	0.47359448549335725	comedy	pets	u7	prefers cat=comedy; globally popular; recent
p29	0.4561095379157588	dance	food	u19	prefers cat=dance; engaged this creator; globally popular; recent
p26	0.3670357648080452	travel	travel	u5	prefers cat=travel; globally popular; recent
p1	0.35579027930345214	education	football	u3	prefers cat=education; globally popular; recent
p18	0.33008250346653223	travel	pets	u4	prefers cat=travel; globally popular; recent
p31	0.32530977442029374	comedy	music	u5	prefers cat=comedy; globally popular; recent
p33	0.3167186352448863	dance	fitness	u16	prefers cat=dance; engaged this creator; globally popular; recent
p28	0.29216389208546306	travel	makeup	u6	prefers cat=travel; recent




User_3 Recommendations
ping_id	score	category	main_hashtag	creator_id	    reason
p20	0.5941102890554548	education	travel	u14	prefers cat=education; engaged this creator; globally popular; recent
p7	0.5509713293275628	travel	music	u8	prefers cat=travel; engaged this creator; globally popular; recent
p25	0.5170488740638747	beauty	makeup	u11	prefers cat=beauty; globally popular; recent
p32	0.4808060239548957	comedy	pets	u7	prefers cat=comedy; globally popular; recent
p29	0.44985953791575883	dance	food	u19	prefers cat=dance; engaged this creator; globally popular; recent
p17	0.4381419021990328	travel	comedy	u17	prefers cat=travel; engaged this creator; globally popular; recent
p36	0.3595457890061826	dance	music	u17	prefers cat=dance; engaged this creator; globally popular; recent
p26	0.34299730326958366	travel	travel	u5	prefers cat=travel; globally popular; recent
p13	0.33903248446217954	food	makeup	u18	prefers cat=food; engaged this creator; globally popular; recent
p31	0.3325213128818322	comedy	music	u5	prefers cat=comedy; globally popular; recent


Evaluation 
Primary offline metrics
•	HitRate@K (Recall@K) — fraction of users for whom Top-K contains an item they actually had a high-engagement event on (view with watch_time_ratio>=0.5, like, share, comment) in the test window. Easy to compute and interpretable.
•	NDCG@K — uses graded relevance (e.g., share=4, comment=3, like=2, view with ratio bucketed) to reward higher-ranked hits more.
•	MRR@K — useful if you care about first-hit position.
•	Coverage & catalog churn — fraction of items recommended at least once; measure recommendation diversity / long-tail exposure.
•	Novelty / freshness — fraction of recommendations that are new to the user (not previously seen).
•	Calibration / fairness metrics — check popularity bias and creator exposure; ensure creators get fair impressions.
Online metrics (A/B)
•	Watch-through rate (per-recommendation): % of views with watch_time_ratio >= 0.8.
•	Engagement rate per impression (views+likes+shares+comments / impressions).
•	Retention / session length — does the recommender increase session length or DAU/WAU?
•	Creator retention / monetization signals — follows, shares.

Trade-offs & engineering considerations
•	Freshness vs relevance: boosting freshness helps discovery but can reduce immediate engagement (if fresh items are low-quality). Tune delta (freshness weight) and consider contextual buckets (new-user vs veteran).
•	Popularity bias vs personalization: heavy global_pop increases short-term engagement and reduces risk (safe), but amplifies popular creators and hurts long-tail discovery. Use hybrid weighting or diversify candidates.
•	Explainability vs accuracy: simple linear heuristics are explainable and cheap; advanced models (LightGBM, XGBoost, Transformers) may yield higher offline metrics but require more infra and are harder to debug.
•	Cold-start: new items and new users need special handling (impression seeding, default popularity, content-based features, or exploration bucket).
•	Latency vs complexity: heavy ranking models can be used only in the second-stage (re-ranking) — retrieval must remain fast (Two-Tower / ANN index or precomputed candidate sets).
•	Evaluation temporal split: always use time-based train/test splits (train on past, test on future) to avoid leakage.
Practical rollout plan
1.	Start with the heuristic baseline (this pipeline). Run offline eval & small A/B test (on a small percentage of traffic) vs current system.
2.	If uplift is promising, iterate with feature engineering and small gradient-boosted model (LightGBM / XGBoost) as re-ranker.
3.	For large catalogs & scale, implement a Two-Tower retrieval (user/item embeddings) + ANN (FAISS/ScaNN) as retrieval and a learned ranker for final sort. Monitor cost and latency


Scalable Architecture (AWS)
Design goals: Low cost, scalable, simple to operate. Favor managed services, batch retraining, and a lightweight real-time serving tier.
1) Event collection & storage
•	Mobile clients → HTTPS → API Gateway.
•	API Gateway → Lambda (validation + auth) → Kinesis Data Firehose (or Data Streams).
•	Firehose writes raw events to S3 (Parquet) for durable, cheap storage and audit logs.
•	For near-real-time aggregation, stream a copy to Kinesis Data Streams (or small MSK) → consumer (Lambda / Kinesis Data Analytics) that writes aggregates to DynamoDB or ElastiCache (Redis).
2) Training & feature pipelines
•	Batch ETL: AWS Glue (or Spark on EMR) / Athena jobs read S3 parquet → build training tables (user, item features).
•	Model training: SageMaker or AWS Batch for heavier models; Glue/Athena + simple Lambda for heuristic baselines.
•	Retraining cadence: cost-aware defaults: nightly or every 24–72 hours. Maintain an hourly lightweight job that updates popularity/freshness aggregates if needed.
•	Feature store / serving store: store compact features in DynamoDB (cost-efficient, low latency). Use Redis (Elasticache) only if sub-10ms access and higher cost is justified.
3) Real-time serving
•	Request flow: Client → API Gateway → Lambda → fetch candidate list + user features from DynamoDB/Redis → compute ranking (weighted sum or model) in Lambda → return Top-N.
•	Candidate generation: precompute popular / topical candidate lists and per-user candidate sets in DynamoDB; Lambda re-ranks.
•	Latency targets: ranking compute 100–200ms; end-to-end < 300ms (account for cold starts). Use provisioned concurrency for critical endpoints if needed.
4) AWS Personalize vs Custom
•	Personalize: good for fast CF solution. Send events (impression, view, like, comment, share, follow_creator) + item/user attributes (duration_sec, hashtag, category, country, signup_date, event_value for watch ratio).
o	Cost control: use batch recommendations, smaller datasets, simpler recipes, less frequent retraining.
•	Custom (recommended initially): cheaper, more transparent — implement heuristic: score = 0.5*pop + 0.25*creator_aff + 0.15*cat_aff + 0.1*freshness.
o	Iterate to SageMaker logistic/GBM re-rankers if offline eval justifies cost.
5) Cost controls & ops tips
•	Run heavy compute offline and in off-hours (Glue/EMR/SageMaker spot).
•	Keep feature vectors compact; use DynamoDB with autoscaling or on-demand capacity.
•	Use S3 + Athena for ad-hoc analytics.
•	Limit real-time model complexity; use two-stage architecture (cheap retrieval + expensive re-rank only for top candidates).
•	Monitor cost vs. uplift; delay expensive managed services until clear ROI.

 Dashboard metrics + example SQL
Key metrics for a “Recommendation System Health” dashboard and example SQL 
1.	Watch-through rate: fraction of view events with watch_time_ratio >= 0.8.
SELECT
  SUM(CASE WHEN event_type='view' AND (watch_time_sec/duration_sec) >= 0.8 THEN 1 ELSE 0 END) *1.0 /
  SUM(CASE WHEN event_type='view' THEN 1 ELSE 0 END) AS watch_through_rate
FROM interactions;
2.	Click/engagement rate per impression (views+likes+shares+comments per impression):
SELECT
  SUM(CASE WHEN event_type IN ('view','like','comment','share') THEN 1 ELSE 0 END) *1.0 /
  SUM(CASE WHEN event_type = 'impression' THEN 1 ELSE 0 END) AS engagement_per_impression
FROM interactions;
3.	HitRate@10 (offline): for an offline evaluation table recs(uid, ping_id, rank) and test_interactions, compute fraction of users with at least one high engagement in top 10.
-- pseudo-SQL, depends on materialized tables
SELECT AVG(CASE WHEN hit_count > 0 THEN 1 ELSE 0 END) AS hitrate_at_10
FROM (
  SELECT r.user_id, COUNT(*) FILTER (WHERE t.event_type IN ('share','comment') OR (t.event_type='view' AND t.watch_time_sec/t.duration_sec>=0.5)) AS hit_count
  FROM recs r
  LEFT JOIN test_interactions t ON r.user_id = t.user_id AND r.ping_id = t.ping_id
  WHERE r.rank <= 10
  GROUP BY r.user_id
) s;
4.	Average time-to-first-engagement after impression (median):
-- needs sessionization: find impression timestamp and subsequent first view/like by same user/ping
5.	Item coverage: fraction of items ever recommended / fraction of catalog items:
SELECT
  COUNT(DISTINCT ping_id) FILTER (WHERE recommended_flag=1) *1.0 / COUNT(DISTINCT ping_id) AS coverage
FROM item_table;
6.	Top failure modes: % of impressions with zero subsequent views within 24 hours:
-- join impressions with any view event within 24h, count the misses



